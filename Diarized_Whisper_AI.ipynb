{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrissyhroberts/Diarization-WhisperAI/blob/main/Diarized_Whisper_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3DvVwYV0m91"
      },
      "source": [
        "# Diarised transcriptions and translations using Pyannote & Whisper AI.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfvIZ67J3Brq"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW3W0zEVxCrZ"
      },
      "source": [
        "### Install required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUAYiW1zxT-V"
      },
      "outputs": [],
      "source": [
        "!pip install pydub\n",
        "!pip install light-the-torch\n",
        "!ltt install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1\n",
        "!pip install  git+https://github.com/hmmlearn/hmmlearn.git\n",
        "!pip install  git+https://github.com/pyannote/pyannote-audio.git@develop\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzAtNEblwk00"
      },
      "source": [
        "### Import all libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtovrtlGwtst"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "from google.colab import drive\n",
        "import json\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "from pathlib import Path\n",
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "import re\n",
        "import torch\n",
        "import whisper\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtlCXpHr2pkQ"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3tqC8m8fyCJ"
      },
      "outputs": [],
      "source": [
        "drive_mount_path = Path(\"/content/drive\")\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"MyDrive\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtljXaTXnowa"
      },
      "source": [
        "*italicised text*# Preparing the audio file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7TPgEVW8XeH"
      },
      "source": [
        "\n",
        "## Specify details of run\n",
        "\n",
        "This script assumes that you have the source file (audio/video) saved on your google drive.\n",
        "\n",
        "* Enter the full path to the source file on the `video_path` variable.\n",
        " * The google drive path must start /content/drive/MyDrive/...\n",
        "* The `output_path` variable should be the full path to a folder where the files should be saved\n",
        " * This will also start /content/drive/MyDrive/...\n",
        " * If it doesn't exist, the script will create it\n",
        "* The `access_token` is a requirement of the usage conditions for the diarisation software.\n",
        " * **Important:** To load the pyannote speaker diarization pipeline,\n",
        "you must first accept the user conditions on both [hf.co/pyannote/speaker-diarization](https://hf.co/pyannote/speaker-diarization) and [hf.co/pyannote/segmentation](https://huggingface.co/pyannote/segmentation).\n",
        " * You'll need to create a huggingface account, then create an access token.\n",
        " * Then paste your access_token or login using `notebook_login` in the access_token variable below\n",
        " * Copy a token from your [Hugging Face tokens page](https://huggingface.co/settings/tokens) and paste it in the `access_token` box.\n",
        "\n",
        "* The `audio_title` variable is simply the name of the document. This will be shown at the top of the html output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zqnZsBacKph"
      },
      "outputs": [],
      "source": [
        "Source = 'File (Google Drive)'\n",
        "#store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown #### **Google Drive video or audio path (mp4, wav, mp3)**\n",
        "video_path = \"/content/drive/MyDrive/Colab_Notebooks/Whisper/harry_sally.m4a\" #@param {type:\"string\"}\n",
        "output_path = \"/content/drive/MyDrive/Colab_Notebooks/Whisper/content/harry_sally\" #@param {type:\"string\"}\n",
        "output_path = str(Path(output_path))\n",
        "audio_title = \"When Harry Met Sally\" #@param {type:\"string\"}\n",
        "access_token = \"hf_zzmcacajGBJoasdalkHZmbYZkBMGSmGW\" #@param {type:\"string\"}\n",
        "language_source = 'English' #@param ['any','Afrikaans','Arabic','Armenian','Azerbaijani','Belarusian','Bosnian','Bulgarian','Catalan','Chinese','Croatian','Czech','Danish','Dutch','English','Estonian','Finnish','French','Galician','German','Greek','Hebrew','Hindi','Hungarian','Icelandic','Indonesian','Italian','Japanese','Kannada','Kazakh','Korean','Latvian','Lithuanian','Macedonian','Malay','Marathi','Maori','Nepali','Norwegian','Persian','Polish','Portuguese','Romanian','Russian','Serbian','Slovak','Slovenian','Spanish','Swahili','Swedish','Tagalog','Tamil','Thai','Turkish','Ukrainian','Urdu','Vietnamese','Welsh']\n",
        "whisper_task = 'transcribe' #@param ['transcribe','translate']\n",
        "model_size = 'medium' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
        "#append output path with task type\n",
        "output_path = os.path.join(output_path, whisper_task)\n",
        "# Assuming video_path is defined somewhere above\n",
        "video_base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "print(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2qTkKD_30FG"
      },
      "source": [
        "### Define Speakers\n",
        "\n",
        "Change or add to the speaker names and collors bellow as you wish `(speaker, textbox color, speaker color)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7EP6fO73wTY"
      },
      "outputs": [],
      "source": [
        "speakers = {\n",
        "    'SPEAKER_00':('Speaker 01', '#e1ffc7', 'darkgreen'),\n",
        "    'SPEAKER_01':('Speaker 02', 'white', 'darkorange'),\n",
        "    'SPEAKER_02':('Speaker 03', 'yellow','darkblue'),\n",
        "    'SPEAKER_03':('Speaker 04', 'green','black'),\n",
        "    'SPEAKER_04':('Speaker 05', 'orange','darkblue')\n",
        "    }\n",
        "def_boxclr = 'white'\n",
        "def_spkrclr = 'orange'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_bYuLyB6ZP_"
      },
      "source": [
        "## Prepare data and folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o98zPx4Pzzwv"
      },
      "source": [
        "### Make an output folder based on `output_path`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vgK82ahXNje"
      },
      "outputs": [],
      "source": [
        "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "%cd {output_path}\n",
        "video_title = \"\"\n",
        "video_id = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o84wUnGr5Nql"
      },
      "source": [
        "### Re-encode audio stream for input to pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjORT6CkVoTF"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -i {repr(video_path)} -vn -acodec pcm_s16le -ar 16000 -ac 1 -y input.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u1vbqd_VzNp"
      },
      "source": [
        "### Prepending a spacer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7qMLTISFE6M"
      },
      "source": [
        "`pyannote.audio` seems to miss the first 0.5 seconds of the audio, and, therefore, we prepend a spcacer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaRDsBV1CWi8"
      },
      "outputs": [],
      "source": [
        "\n",
        "spacermilli = 2000\n",
        "spacer = AudioSegment.silent(duration=spacermilli)\n",
        "\n",
        "\n",
        "audio = AudioSegment.from_wav(\"input.wav\")\n",
        "\n",
        "audio = spacer.append(audio, crossfade=0)\n",
        "\n",
        "audio.export('input_prep.wav', format='wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb5eEOKUooju"
      },
      "source": [
        "## Diarization using Pyannote"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxNf1l8Ye_U9"
      },
      "source": [
        "[`pyannote.audio`](https://github.com/pyannote/pyannote-audio) is an open-source toolkit written in Python for **speaker diarization**.\n",
        "\n",
        "Based on [`PyTorch`](https://pytorch.org) machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines.\n",
        "\n",
        "`pyannote.audio` also comes with pretrained [models](https://huggingface.co/models?other=pyannote-audio-model) and [pipelines](https://huggingface.co/models?other=pyannote-audio-pipeline) covering a wide range of domains for voice activity detection, speaker segmentation, overlapped speech detection, speaker embedding reaching state-of-the-art performance for most of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Ak_OQwqd-3"
      },
      "source": [
        "### Define Pyannote pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKG14DGYbwku"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization', use_auth_token= (access_token) or True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9IS3yKZRys-"
      },
      "outputs": [],
      "source": [
        "pipeline.parameters(instantiated=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3r0SNDsY1LE"
      },
      "source": [
        "There is no reason the above hyper-parameters are optimal for the newly finetuned speaker segmentation model. Let's optimize them:\n",
        "\n",
        "* segmentation.threshold\n",
        "  * in the technical report, between 0 and 1) controls the aggressiveness of speaker activity detection (i.e. a higher value will result in less detected speech);\n",
        "\n",
        "* clustering.threshold\n",
        "  * in the report, between 0 and 2) controls the number of speakers (i.e. a higher value will result in less speakers).\n",
        "\n",
        "* segmentation.min_duration_off\n",
        "\n",
        "  * in the report, in seconds) controls whether intra-speaker pauses are filled. This usually depends on the downstream application so it is better to first force it to zero (i.e. never fill intra-speaker pauses) during optimization.\n",
        "\n",
        "* clustering.centroid\n",
        "  * is the linkage used by the agglomerative clustering step. centroid has been found to be slightly better than average.\n",
        "\n",
        "* clustering.min_cluster_size\n",
        "  * controls what to do with small speaker clusters. Clusters smaller than that are assigned to the most similar large cluster. 15 is a good default value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfJZWhM2TXOX"
      },
      "outputs": [],
      "source": [
        "new_params = {'segmentation': {\n",
        "                              'min_duration_off': 0.5817029604921046,\n",
        "                              'threshold': 0.4442333667381752\n",
        "                              },\n",
        "\n",
        "                  'clustering': {\n",
        "                              'method': 'centroid',\n",
        "                              'min_cluster_size': 15,\n",
        "                              'threshold': 0.7153814381597874\n",
        "                              }\n",
        "              }\n",
        "pipeline.instantiate(new_params)\n",
        "pipeline.parameters(instantiated=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOX2VlP16BPx"
      },
      "source": [
        "### Define whether GPU or CPU is going to be used.\n",
        "Almost not worth bothering with CPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjsUVdR2jH0n"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipeline.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImKMcCr5W5Nw"
      },
      "source": [
        "### Run pyannote.audio to generate the diarizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA4xiEefft9Z"
      },
      "outputs": [],
      "source": [
        "DEMO_FILE = {'uri': 'blabla', 'audio': 'input_prep.wav'}\n",
        "dz = pipeline(DEMO_FILE)\n",
        "#dz = pipeline(DEMO_FILE, num_speakers = 2)\n",
        "\n",
        "with open(\"diarization.txt\", \"w\") as text_file:\n",
        "    text_file.write(str(dz))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHIY2MB3Vz3e"
      },
      "outputs": [],
      "source": [
        "print(*list(dz.itertracks(yield_label = True))[:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp36eMedRkR0"
      },
      "source": [
        "### Prepare trimmed audio files according to the diarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPGOaVpOH7pZ"
      },
      "outputs": [],
      "source": [
        "def millisec(timeStr):\n",
        "  spl = timeStr.split(\":\")\n",
        "  s = (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2]) )* 1000)\n",
        "  return s\n",
        "\n",
        "# Function to format time in HH:MM:SS.sss format\n",
        "def timeStr(t):\n",
        "    return '{0:02d}:{1:02d}:{2:06.2f}'.format(round(t // 3600),\n",
        "                                               round(t % 3600 // 60),\n",
        "                                               t % 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Co3BIIH6aW4"
      },
      "source": [
        "### Group the diarization segments according to the speaker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umQdzNFzcP2f"
      },
      "outputs": [],
      "source": [
        "dzs = open('diarization.txt').read().splitlines()\n",
        "\n",
        "groups = []\n",
        "g = []\n",
        "lastend = 0\n",
        "\n",
        "for d in dzs:\n",
        "  if g and (g[0].split()[-1] != d.split()[-1]):      #same speaker\n",
        "    groups.append(g)\n",
        "    g = []\n",
        "\n",
        "  g.append(d)\n",
        "\n",
        "  end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=d)[1]\n",
        "  end = millisec(end)\n",
        "  if (lastend > end):       #segment engulfed by a previous segment\n",
        "    groups.append(g)\n",
        "    g = []\n",
        "  else:\n",
        "    lastend = end\n",
        "if g:\n",
        "  groups.append(g)\n",
        "print(*groups, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOuf8CuRQeZo"
      },
      "source": [
        "### Save the audio part corresponding to each diarization group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRQPUW4Mzvfn"
      },
      "outputs": [],
      "source": [
        "audio = AudioSegment.from_wav(\"input_prep.wav\")\n",
        "gidx = -1\n",
        "for g in groups:\n",
        "  start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
        "  end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
        "  start = millisec(start) #- spacermilli\n",
        "  end = millisec(end)  #- spacermilli\n",
        "  gidx += 1\n",
        "  #audio[start:end].export(str(gidx) + '.wav', format='wav')\n",
        "  #print(f\"group {gidx}: {start}--{end}\")\n",
        "  audio[start:end].export(str(gidx) + '.mp3', format='mp3')\n",
        "  print(f\"group {gidx}: {start}--{end}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv2GYZCsLKBJ"
      },
      "source": [
        "### Free up some memory by removing some variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cONumKWUjfus"
      },
      "outputs": [],
      "source": [
        "del   DEMO_FILE, pipeline, spacer,  audio, dz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmxtB0k4n8lY"
      },
      "source": [
        "# Transcription & Translation using Whisper AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TGTZHeg7f73"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHKf0tFVmGyq"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = whisper.load_model(model_size, device = device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBO8IpdiRQ0X"
      },
      "source": [
        "### Run whisper on all audio files. Whisper generates the transcription and writes it to a file.\n",
        "This can take a long time, depending on your input file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odstu62EnMLL"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(len(groups)):\n",
        "    audiof = str(i) + '.mp3'\n",
        "\n",
        "    if whisper_task == 'transcribe':\n",
        "        # Perform transcription\n",
        "        result = model.transcribe(audio=audiof, language=language_source, word_timestamps=True)\n",
        "    elif whisper_task == 'translate':\n",
        "        # Perform translation\n",
        "        result = model.transcribe(audio=audiof, language=language_source, word_timestamps=True, task = \"translate\")\n",
        "    else:\n",
        "        raise ValueError(\"Invalid whisper_task value. Use 'transcribe' or 'translate'.\")\n",
        "\n",
        "    # Save the result to a JSON file\n",
        "    with open(str(i) + '.json', \"w\") as outfile:\n",
        "        json.dump(result, outfile, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_UyWQMXpB3N"
      },
      "source": [
        "### Generate the HTML and/or txt file from the Transcriptions and the Diarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KndDYy_xMpMq"
      },
      "source": [
        "In the generated HTML,  the transcriptions for each diarization group are written in a box, with the speaker name on the top. By clicking a transcription, the embedded video jumps to the right time ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKdx9Hwg630K"
      },
      "outputs": [],
      "source": [
        "preS = '\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<meta name=\"viewport\" content=\"whtmlidth=device-width, initial-scale=1.0\">\\n\\t<meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\\n\\t<title>' + \\\n",
        "    audio_title+ \\\n",
        "    '</title>\\n\\t<style>\\n\\t\\tbody {\\n\\t\\t\\tfont-family: sans-serif;\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\tcolor: #111;\\n\\t\\t\\tpadding: 0 0 1em 0;\\n\\t\\t\\tbackground-color: #efe7dd;\\n\\t\\t}\\n\\n\\t\\ttable {\\n\\t\\t\\tborder-spacing: 10px;\\n\\t\\t}\\n\\n\\t\\tth {\\n\\t\\t\\ttext-align: left;\\n\\t\\t}\\n\\n\\t\\t.lt {\\n\\t\\t\\tcolor: inherit;\\n\\t\\t\\ttext-decoration: inherit;\\n\\t\\t}\\n\\n\\t\\t.l {\\n\\t\\t\\tcolor: #050;\\n\\t\\t}\\n\\n\\t\\t.s {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.c {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.e {\\n\\t\\t\\t/*background-color: white; Changing background color */\\n\\t\\t\\tborder-radius: 10px;\\n\\t\\t\\t/* Making border radius */\\n\\t\\t\\twidth: 50%;\\n\\t\\t\\t/* Making auto-sizable width */\\n\\t\\t\\tpadding: 0 0 0 0;\\n\\t\\t\\t/* Making space around letters */\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\t/* Changing font size */\\n\\t\\t\\tmargin-bottom: 0;\\n\\t\\t}\\n\\n\\t\\t.t {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t#player-div {\\n\\t\\t\\tposition: sticky;\\n\\t\\t\\ttop: 20px;\\n\\t\\t\\tfloat: right;\\n\\t\\t\\twidth: 40%\\n\\t\\t}\\n\\n\\t\\t#player {\\n\\t\\t\\taspect-ratio: 16 / 9;\\n\\t\\t\\twidth: 100%;\\n\\t\\t\\theight: auto;\\n\\t\\t}\\n\\n\\t\\ta {\\n\\t\\t\\tdisplay: inline;\\n\\t\\t}\\n\\t</style>';\n",
        "preS += '\\n\\t<script>\\n\\twindow.onload = function () {\\n\\t\\t\\tvar player = document.getElementById(\"audio_player\");\\n\\t\\t\\tvar player;\\n\\t\\t\\tvar lastword = null;\\n\\n\\t\\t\\t// So we can compare against new updates.\\n\\t\\t\\tvar lastTimeUpdate = \"-1\";\\n\\n\\t\\t\\tsetInterval(function () {\\n\\t\\t\\t\\t// currentTime is checked very frequently (1 millisecond),\\n\\t\\t\\t\\t// but we only care about whole second changes.\\n\\t\\t\\t\\tvar ts = (player.currentTime).toFixed(1).toString();\\n\\t\\t\\t\\tts = (Math.round((player.currentTime) * 5) / 5).toFixed(1);\\n\\t\\t\\t\\tts = ts.toString();\\n\\t\\t\\t\\tconsole.log(ts);\\n\\t\\t\\t\\tif (ts !== lastTimeUpdate) {\\n\\t\\t\\t\\t\\tlastTimeUpdate = ts;\\n\\n\\t\\t\\t\\t\\t// Its now up to you to format the time.\\n\\t\\t\\t\\t\\tword = document.getElementById(ts)\\n\\t\\t\\t\\t\\tif (word) {\\n\\t\\t\\t\\t\\t\\tif (lastword) {\\n\\t\\t\\t\\t\\t\\t\\tlastword.style.fontWeight = \"normal\";\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tlastword = word;\\n\\t\\t\\t\\t\\t\\t//word.style.textDecoration = \"underline\";\\n\\t\\t\\t\\t\\t\\tword.style.fontWeight = \"bold\";\\n\\n\\t\\t\\t\\t\\t\\tlet toggle = document.getElementById(\"autoscroll\");\\n\\t\\t\\t\\t\\t\\tif (toggle.checked) {\\n\\t\\t\\t\\t\\t\\t\\tlet position = word.offsetTop - 20;\\n\\t\\t\\t\\t\\t\\t\\twindow.scrollTo({\\n\\t\\t\\t\\t\\t\\t\\t\\ttop: position,\\n\\t\\t\\t\\t\\t\\t\\t\\tbehavior: \"smooth\"\\n\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}, 0.1);\\n\\t\\t}\\n\\n\\t\\tfunction jumptoTime(timepoint, id) {\\n\\t\\t\\tvar player = document.getElementById(\"audio_player\");\\n\\t\\t\\thistory.pushState(null, null, \"#\" + id);\\n\\t\\t\\tplayer.pause();\\n\\t\\t\\tplayer.currentTime = timepoint;\\n\\t\\t\\tplayer.play();\\n\\t\\t}\\n\\t\\t</script>\\n\\t</head>';\n",
        "preS += '\\n\\n<body>\\n\\t<h2>' + audio_title + '</h2>\\n\\t<i>Click on a part of the transcription, to jump to its portion of audio, and get an anchor to it in the address\\n\\t\\tbar<br><br></i>\\n\\t<div id=\"player-div\">\\n\\t\\t<div id=\"player\">\\n\\t\\t\\t<audio controls=\"controls\" id=\"audio_player\">\\n\\t\\t\\t\\t<source src=\"input.wav\" />\\n\\t\\t\\t</audio>\\n\\t\\t</div>\\n\\t\\t<div><label for=\"autoscroll\">auto-scroll: </label>\\n\\t\\t\\t<input type=\"checkbox\" id=\"autoscroll\" checked>\\n\\t\\t</div>\\n\\t</div>\\n';\n",
        "\n",
        "postS = '\\t</body>\\n</html>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx_TpyGAk30v"
      },
      "source": [
        "## Print the transcripts to files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuMapB9Ok3PS"
      },
      "outputs": [],
      "source": [
        "html = list(preS)\n",
        "txt = list(\"\")\n",
        "gidx = -1\n",
        "\n",
        "for g in groups:\n",
        "  shift = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
        "  shift = millisec(shift) - spacermilli # the start time in the original video\n",
        "  shift = max(shift, 0)\n",
        "\n",
        "  gidx += 1\n",
        "\n",
        "  captions = json.load(open(str(gidx) + '.json'))['segments']\n",
        "\n",
        "  if captions:\n",
        "    speaker = g[0].split()[-1]\n",
        "    boxclr = def_boxclr\n",
        "    spkrclr = def_spkrclr\n",
        "    if speaker in speakers:\n",
        "      speaker, boxclr, spkrclr = speakers[speaker]\n",
        "\n",
        "    for c in captions:\n",
        "      start = shift + c['start'] * 1000.0\n",
        "      start = start / 1000.0\n",
        "      end = (shift + c['end'] * 1000.0) / 1000.0\n",
        "      txt.append(f'[{timeStr(start)} --> {timeStr(end)}]\\t[{speaker}]\\t{c[\"text\"].lstrip()}\\n')\n",
        "\n",
        "      html.append(f'<div class=\"e\" style=\"background-color: {boxclr}\">\\n')\n",
        "      html.append('<p  style=\"margin:0;padding: 5px 10px 10px 10px;word-wrap:normal;white-space:normal;\">\\n')\n",
        "      html.append(f'<span style=\"color:{spkrclr};font-weight: bold;\">{speaker}</span><br>\\n\\t\\t\\t\\t')\n",
        "\n",
        "      for i, w in enumerate(c['words']):\n",
        "        if w == \"\":\n",
        "          continue\n",
        "        start = (shift + w['start']*1000.0) / 1000.0\n",
        "        html.append(f'<a href=\"#{timeStr(start)}\" id=\"{\"{:.1f}\".format(round(start*5)/5)}\" class=\"lt\" onclick=\"jumptoTime({int(start)}, this.id)\">{w[\"word\"]}</a><!--\\n\\t\\t\\t\\t-->')\n",
        "\n",
        "      html.append('</p>\\n')\n",
        "      html.append('</div>\\n')\n",
        "\n",
        "html.append(postS)\n",
        "\n",
        "with open(f\"00_{video_base_name}_transcript.txt\", \"w\", encoding='utf-8') as file:\n",
        "  s = \"\".join(txt)\n",
        "  file.write(s)\n",
        "  print(f'captions saved to 00_{video_base_name}_transcript.txt:')\n",
        "  print(s+'\\n')\n",
        "\n",
        "with open(f\"00_{video_base_name}_transcript.html\", \"w\", encoding='utf-8') as file:\n",
        "  s = \"\\t\".join(html)\n",
        "  file.write(s)\n",
        "  print(f'captions saved to 00_{video_base_name}_transcript.html:')\n",
        "  print(s+'\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuZpGhUsplV1"
      },
      "outputs": [],
      "source": [
        "print(txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekmMPKwpxxEv"
      },
      "source": [
        "### Add a full transcript to the outputs, fully diarized in .txt format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQyROdrfsvk4"
      },
      "source": [
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/Majdoddin/nlp/blob/main/Pyannote_plays_and_Whisper_rhymes_v_2_0.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/majdoddin/nlp)\n",
        "\n",
        "# Whisper's transcription plus Pyannote's Diarization\n",
        "\n",
        "**Update** - [@johnwyles](https://github.com/johnwyles) added HTML output for audio/video files from Google Drive, along with some fixes.\n",
        "\n",
        "Using the new word-level timestamping of Whisper, the transcription words are highlighted as the video plays, with optional autoscroll. And the display on small displays is improved.\n",
        "\n",
        "Moreover, the model is loaded just once, thus the whole thing runs much faster now. You can also hardcode your Huggingface token.\n",
        "\n",
        "---\n",
        "Andrej Karpathy [suggested](https://twitter.com/karpathy/status/1574476200801538048?s=20&t=s5IMMXOYjBI6-91dib6w8g) training a classifier on top of  OpenAI [Whisper](https://openai.com/blog/whisper/) model features to identify the speaker, so we can visualize the speaker in the transcript. But, as [pointed out](https://twitter.com/tarantulae/status/1574493613362388992?s=20&t=s5IMMXOYjBI6-91dib6w8g) by Christian Perone, it seems that features from whisper wouldn't be that great for speaker recognition as its main objective is basically to ignore speaker differences.\n",
        "\n",
        "In the following, I use [**`pyannote-audio`**](https://github.com/pyannote/pyannote-audio), a speaker diarization toolkit by Hervé Bredin, to identify the speakers, and then match it with the transcriptions of Whispr, linked to the video. The input can be YouTube or an video/audio file (also on Google Drive). I try it on a [Customer Support Call](https://youtu.be/hpZFJctBUHQ). Check the result [**here**](https://majdoddin.github.io/dyson.html).\n",
        "\n",
        "To make it easier to match the transcriptions to diarizations by speaker change, Sarah Kaiser [suggested](https://github.com/openai/whisper/discussions/264#discussioncomment-3825375) runnnig the pyannote.audio first and  then just running whisper on the split-by-speaker chunks.\n",
        "For sake of performance (and transcription quality?), we attach the audio segments into a single audio file with a silent spacer as a separator, and run whisper on it. Enjoy it!\n",
        "\n",
        "(For sake of performance , I also tried attaching the audio segments into a single audio file with a silent -or beep- spacer as a separator, and run whisper on it see it on [colab](https://colab.research.google.com/drive/1HuvcY4tkTHPDzcwyVH77LCh_m8tP-Qet?usp=sharing). It [works](https://majdoddin.github.io/lexicap.html) on some audio, and fails on some (Dyson's Interview). The problem is, whisper does not reliably make a timestap on a spacer. See the discussions [#139](https://github.com/openai/whisper/discussions/139) and [#29](https://github.com/openai/whisper/discussions/29))\n",
        "\n",
        "The Markdown form used below is from [@ArthurFDLR](https://github.com/ArthurFDLR/whisper-youtube/).   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf6yDSZZMc6t"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# ... (your existing code)\n",
        "\n",
        "# Initialize a figure\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Get unique speaker IDs\n",
        "unique_speakers = set()\n",
        "for group in groups:\n",
        "    speaker_id = group[0].split()[-1]\n",
        "    unique_speakers.add(speaker_id)\n",
        "\n",
        "# Assign a unique y-value to each speaker ID\n",
        "speaker_to_y = {speaker_id: i for i, speaker_id in enumerate(unique_speakers)}\n",
        "\n",
        "# Loop through the groups and plot segments\n",
        "for group in groups:\n",
        "    start_time_str = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=group[0])[0]\n",
        "    end_time_str = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=group[-1])[1]\n",
        "\n",
        "    start_time = millisec(start_time_str)\n",
        "    end_time = millisec(end_time_str)\n",
        "\n",
        "    duration = end_time - start_time\n",
        "    speaker_id = group[0].split()[-1]\n",
        "    y_value = speaker_to_y[speaker_id]\n",
        "\n",
        "    # Create a rectangle patch to represent the segment\n",
        "    rect = patches.Rectangle((start_time, y_value), duration, 0.8, linewidth=1, edgecolor='r', facecolor='lightgray')\n",
        "\n",
        "    # Add the patch to the axes\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    # Add a text label to the segment\n",
        "    ax.text(start_time + duration / 2, y_value + 0.4, speaker_id, horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "# Set the labels and title\n",
        "ax.set_ylabel('Speaker ID')\n",
        "ax.set_xlabel('Time (ms)')\n",
        "ax.set_title('Speaker Diarization')\n",
        "ax.set_yticks(range(len(unique_speakers)))\n",
        "ax.set_yticklabels(unique_speakers)\n",
        "\n",
        "# Set the axis limits\n",
        "ax.set_xlim(0, end_time + 1000)  # Adding some extra space at the end\n",
        "ax.set_ylim(-1, len(unique_speakers))\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nfvIZ67J3Brq",
        "OW3W0zEVxCrZ",
        "V2qTkKD_30FG",
        "z_bYuLyB6ZP_",
        "aOX2VlP16BPx",
        "AmxtB0k4n8lY",
        "7TGTZHeg7f73"
      ],
      "gpuType": "V100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
